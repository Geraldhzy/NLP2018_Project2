{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('word2vec_300d.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_puncts(data):\n",
    "    data = re.sub(\"<e1>|</e1>|<e2>|</e2>\", \"\", data)\n",
    "    data = re.sub(\"[0-9\\<\\>\\.\\!\\/_,~@#$&%^*\\:\\?()\\+\\-\\=\\\"\\']\", \" \", data.lower())\n",
    "    return data\n",
    "\n",
    "def tokenize(data):\n",
    "    data = word_tokenize(data)\n",
    "    return data\n",
    "\n",
    "def clear_stopwords(data):\n",
    "    english_stopwords = stopwords.words(\"english\")\n",
    "    text = []\n",
    "    for i, w in enumerate(data):\n",
    "        if w not in english_stopwords:\n",
    "            text.append(w)\n",
    "    return text\n",
    "\n",
    "def preprocess(data):\n",
    "    data = clear_puncts(data)\n",
    "    data = tokenize(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {}\n",
    "with open (\"TEST_FILE.txt\") as f:\n",
    "    for line in f:\n",
    "        l = line.split('\\t')\n",
    "        idx = l[0]\n",
    "        test_data[idx] = {}\n",
    "        text = l[1]\n",
    "        e1_big = text.find('<e1>')\n",
    "        e1_end = text.find('</e1>')\n",
    "        e2_big = text.find('<e2>')\n",
    "        e2_end = text.find('</e2>')\n",
    "        e1 = text[e1_big + 4: e1_end]\n",
    "        e2 = text[e2_big + 4: e2_end]\n",
    "        test_data[idx]['text'] = preprocess(text)\n",
    "        test_data[idx]['e1'] = e1\n",
    "        test_data[idx]['e2'] = e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {}\n",
    "with open(\"TRAIN_FILE.txt\") as f:\n",
    "    for i in range(8000):\n",
    "        line = f.readline()\n",
    "        relation = f.readline().strip('\\n')\n",
    "        comment = f.readline()\n",
    "        f.readline()\n",
    "        l = line.split('\\t')\n",
    "        idx = l[0]\n",
    "        text = l[1]\n",
    "        train_data[idx] = {}\n",
    "        e1_big = text.find('<e1>')\n",
    "        e1_end = text.find('</e1>')\n",
    "        e2_big = text.find('<e2>')\n",
    "        e2_end = text.find('</e2>')\n",
    "        e1 = text[e1_big + 4: e1_end]\n",
    "        e2 = text[e2_big + 4: e2_end]\n",
    "        train_data[idx]['text'] = preprocess(text)\n",
    "        train_data[idx]['e1'] = e1\n",
    "        train_data[idx]['e2'] = e2\n",
    "        h = relation.find('(')\n",
    "        t = relation.find(',')\n",
    "        train_data[idx]['rel'] = relation[:h]\n",
    "        train_data[idx]['head'] = relation[h + 1:t]\n",
    "        train_data[idx]['tail'] = relation[t + 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e1': 'child',\n",
       " 'e2': 'cradle',\n",
       " 'head': 'Othe',\n",
       " 'rel': 'Othe',\n",
       " 'tail': 'Othe',\n",
       " 'text': ['the',\n",
       "  'child',\n",
       "  'was',\n",
       "  'carefully',\n",
       "  'wrapped',\n",
       "  'and',\n",
       "  'bound',\n",
       "  'into',\n",
       "  'the',\n",
       "  'cradle',\n",
       "  'by',\n",
       "  'means',\n",
       "  'of',\n",
       "  'a',\n",
       "  'cord']}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e1': 'company',\n",
       " 'e2': 'chairs',\n",
       " 'text': ['the', 'company', 'fabricates', 'plastic', 'chairs']}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['8002']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = {'Cause-Effect':{'e1':0, 'e2':1}, 'Instrument-Agency':{'e1':2, 'e2':3}, 'Product-Producer':{'e1':4, 'e2':5}, 'Content-Container':{'e1':6, 'e2':7}, 'Entity-Origin':{'e1':8, 'e2':9}, 'Entity-Destination':{'e1':10, 'e2':11}, 'Component-Whole':{'e1':12, 'e2':13}, 'Member-Collection':{'e1':14, 'e2':15}, 'Message-Topic':{'e1':16, 'e2':17}, 'Othe':18}\n",
    "inf_classes = {0:'Cause-Effect(e1,e2)', 1:'Cause-Effect(e2,e1)', 2:'Instrument-Agency(e1,e2)', 3:'Instrument-Agency(e2,e1)', 4:'Product-Producer(e1,e2)', 5:'Product-Producer(e2,e1)', 6:'Content-Container(e1,e2)', 7:'Content-Container(e2,e1)', 8:'Entity-Origin(e1,e2)', 9:'Entity-Origin(e2,e1)', 10:'Entity-Destination(e1,e2)', 11:'Entity-Destination(e2,e1)', 12:'Component-Whole(e1,e2)', 13:'Component-Whole(e2,e1)', 14:'Member-Collection(e1,e2)', 15:'Member-Collection(e2,e1)', 16:'Message-Topic(e1,e2)', 17:'Message-Topic(e2,e1)', 18:'Other'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = []\n",
    "train_Y = []\n",
    "test_X = []\n",
    "for d in train_data:\n",
    "    rel = train_data[d]['rel']\n",
    "    if rel == 'Othe':\n",
    "        y = 18\n",
    "    else:\n",
    "        head = train_data[d]['head']\n",
    "        y = classes[rel][head]\n",
    "    train_Y.append(y)\n",
    "    x = 0\n",
    "    for w in train_data[d]['text']:\n",
    "        if w in word_vectors:\n",
    "            x += word_vectors[w]\n",
    "        else:\n",
    "            x += word_vectors['UNK']\n",
    "    x /= len(train_data[d]['text'])\n",
    "    train_X.append(x)\n",
    "for d in test_data:\n",
    "    x = 0\n",
    "    for w in test_data[d]['text']:\n",
    "        if w in word_vectors:\n",
    "            x += word_vectors[w]\n",
    "        else:\n",
    "            x += word_vectors['UNK']\n",
    "    x /= len(test_data[d]['text'])\n",
    "    test_X.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = np.array(train_X)\n",
    "test_X = np.array(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lg = LogisticRegression(solver = 'newton-cg', multi_class = 'multinomial', C = 1)\n",
    "lg.fit(train_X, train_Y)\n",
    "lg.score(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = lg.predict(test_X)\n",
    "with open('wvlg_result.txt', 'w') as f:\n",
    "    for i,y in enumerate(y_pred):\n",
    "        f.write(str(i+8001) + '\\t' + inf_classes[y] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for d in train_data:\n",
    "    if max_len < len(train_data[d]['text']):\n",
    "         max_len = len(train_data[d]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = set()\n",
    "for d in train_data:\n",
    "    for w in train_data[d]['text']:\n",
    "        V.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18703"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "for d in train_data:\n",
    "    rel = train_data[d]['rel']\n",
    "    y = [0]*19\n",
    "    if rel == 'Othe':\n",
    "        y[18] = 1\n",
    "    else:\n",
    "        y[classes[rel][train_data[d]['head']]] = 1\n",
    "    train_y.append(y)\n",
    "    s = []\n",
    "    for w in train_data[d]['text']:\n",
    "        if w in word_vectors:\n",
    "            s.append(word_vectors[w])\n",
    "        else:\n",
    "            s.append(word_vectors['UNK'])\n",
    "    while len(s) < 86:\n",
    "        s.append(np.zeros(300))\n",
    "    train_x.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val = train_x[:800]\n",
    "Y_val = train_y[:800]\n",
    "X = train_x[800:]\n",
    "Y = train_y[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = []\n",
    "for d in test_data:\n",
    "    s = []\n",
    "    for w in test_data[d]['text']:\n",
    "        if w in word_vectors:\n",
    "            s.append(word_vectors[w])\n",
    "        else:\n",
    "            s.append(word_vectors['UNK'])\n",
    "    while len(s) < 86:\n",
    "        s.append(np.zeros(300))\n",
    "    test_x.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "def BLSTM(hidden_size):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=hidden_size, unit_forget_bias=True, implementation=2,\n",
    "                                 activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "                                 kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal'),input_shape=(86,300)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(hidden_size, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "    model.compile('RMSprop', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "model = BLSTM(hidden_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_20 (Bidirectio (None, 256)               439296    \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 19)                2451      \n",
      "=================================================================\n",
      "Total params: 474,643\n",
      "Trainable params: 474,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 13.2003 - acc: 0.1804Epoch 00001: val_acc improved from -inf to 0.12750, saving model to BLSTM.h5\n",
      "7200/7200 [==============================] - 57s 8ms/step - loss: 13.1817 - acc: 0.1804 - val_loss: 9.1340 - val_acc: 0.1275\n",
      "Epoch 2/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 6.8102 - acc: 0.2326Epoch 00002: val_acc improved from 0.12750 to 0.22750, saving model to BLSTM.h5\n",
      "7200/7200 [==============================] - 52s 7ms/step - loss: 6.8018 - acc: 0.2326 - val_loss: 4.7992 - val_acc: 0.2275\n",
      "Epoch 3/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 3.8497 - acc: 0.2796Epoch 00003: val_acc improved from 0.22750 to 0.28500, saving model to BLSTM.h5\n",
      "7200/7200 [==============================] - 51s 7ms/step - loss: 3.8466 - acc: 0.2801 - val_loss: 2.9905 - val_acc: 0.2850\n",
      "Epoch 4/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 2.6510 - acc: 0.3078Epoch 00004: val_acc improved from 0.28500 to 0.32000, saving model to BLSTM.h5\n",
      "7200/7200 [==============================] - 51s 7ms/step - loss: 2.6497 - acc: 0.3079 - val_loss: 2.2840 - val_acc: 0.3200\n",
      "Epoch 5/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 2.2649 - acc: 0.3373Epoch 00005: val_acc improved from 0.32000 to 0.35500, saving model to BLSTM.h5\n",
      "7200/7200 [==============================] - 51s 7ms/step - loss: 2.2653 - acc: 0.3371 - val_loss: 2.2350 - val_acc: 0.3550\n",
      "Epoch 6/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 2.1277 - acc: 0.3563Epoch 00006: val_acc improved from 0.35500 to 0.36875, saving model to BLSTM.h5\n",
      "7200/7200 [==============================] - 51s 7ms/step - loss: 2.1295 - acc: 0.3563 - val_loss: 2.1598 - val_acc: 0.3688\n",
      "Epoch 7/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 2.0680 - acc: 0.3753Epoch 00007: val_acc improved from 0.36875 to 0.40375, saving model to BLSTM.h5\n",
      "7200/7200 [==============================] - 51s 7ms/step - loss: 2.0686 - acc: 0.3753 - val_loss: 2.0396 - val_acc: 0.4037\n",
      "Epoch 8/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 2.0004 - acc: 0.3996Epoch 00008: val_acc did not improve\n",
      "7200/7200 [==============================] - 51s 7ms/step - loss: 2.0003 - acc: 0.3996 - val_loss: 2.1051 - val_acc: 0.3688\n",
      "Epoch 9/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.9589 - acc: 0.4028Epoch 00009: val_acc improved from 0.40375 to 0.43750, saving model to BLSTM.h5\n",
      "7200/7200 [==============================] - 51s 7ms/step - loss: 1.9589 - acc: 0.4025 - val_loss: 1.8889 - val_acc: 0.4375\n",
      "Epoch 10/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.8983 - acc: 0.4189Epoch 00010: val_acc did not improve\n",
      "7200/7200 [==============================] - 51s 7ms/step - loss: 1.8974 - acc: 0.4190 - val_loss: 2.0808 - val_acc: 0.3800\n",
      "Epoch 11/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.8648 - acc: 0.4304Epoch 00011: val_acc improved from 0.43750 to 0.45125, saving model to BLSTM.h5\n",
      "7200/7200 [==============================] - 50s 7ms/step - loss: 1.8637 - acc: 0.4307 - val_loss: 1.8431 - val_acc: 0.4512\n",
      "Epoch 12/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.7979 - acc: 0.4494Epoch 00012: val_acc did not improve\n",
      "7200/7200 [==============================] - 51s 7ms/step - loss: 1.7972 - acc: 0.4493 - val_loss: 1.9056 - val_acc: 0.4275\n",
      "Epoch 13/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.7588 - acc: 0.4636Epoch 00013: val_acc did not improve\n",
      "7200/7200 [==============================] - 51s 7ms/step - loss: 1.7593 - acc: 0.4639 - val_loss: 2.0949 - val_acc: 0.4150\n",
      "Epoch 14/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.7398 - acc: 0.4674Epoch 00014: val_acc did not improve\n",
      "7200/7200 [==============================] - 51s 7ms/step - loss: 1.7394 - acc: 0.4671 - val_loss: 1.8357 - val_acc: 0.4475\n",
      "Epoch 15/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.6812 - acc: 0.4835Epoch 00015: val_acc improved from 0.45125 to 0.45375, saving model to BLSTM.h5\n",
      "7200/7200 [==============================] - 51s 7ms/step - loss: 1.6802 - acc: 0.4839 - val_loss: 1.8746 - val_acc: 0.4537\n",
      "Epoch 16/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.6572 - acc: 0.4930Epoch 00016: val_acc improved from 0.45375 to 0.49625, saving model to BLSTM.h5\n",
      "7200/7200 [==============================] - 51s 7ms/step - loss: 1.6572 - acc: 0.4931 - val_loss: 1.7608 - val_acc: 0.4963\n",
      "Epoch 17/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.6104 - acc: 0.5117Epoch 00017: val_acc did not improve\n",
      "7200/7200 [==============================] - 51s 7ms/step - loss: 1.6105 - acc: 0.5118 - val_loss: 1.8307 - val_acc: 0.4462\n",
      "Epoch 18/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.5742 - acc: 0.5170Epoch 00018: val_acc did not improve\n",
      "7200/7200 [==============================] - 50s 7ms/step - loss: 1.5731 - acc: 0.5171 - val_loss: 1.7491 - val_acc: 0.4637\n",
      "Epoch 19/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.5382 - acc: 0.5289Epoch 00019: val_acc did not improve\n",
      "7200/7200 [==============================] - 51s 7ms/step - loss: 1.5384 - acc: 0.5285 - val_loss: 1.7648 - val_acc: 0.4875\n",
      "Epoch 20/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.5223 - acc: 0.5417Epoch 00020: val_acc did not improve\n",
      "7200/7200 [==============================] - 51s 7ms/step - loss: 1.5218 - acc: 0.5419 - val_loss: 1.7705 - val_acc: 0.4675\n",
      "Epoch 00020: early stopping\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 20\n",
    "batch_size = 256\n",
    "save_path = 'BLSTM.h5'\n",
    "checkpoint = ModelCheckpoint(filepath=save_path,\n",
    "                                     verbose=1,\n",
    "                                     save_best_only=True,\n",
    "                                     save_weights_only=False,\n",
    "                                     monitor='val_acc',\n",
    "                                     mode='max' )\n",
    "csv_logger = CSVLogger('%s-log.csv'%'BLSTM', separator=',', append=False)\n",
    "earlystopping = EarlyStopping(monitor='val_acc', patience = 4, verbose=1, mode='max')\n",
    "history = model.fit(X, Y,\n",
    "                      validation_data=(X_val, Y_val),\n",
    "                      epochs=nb_epoch,\n",
    "                      batch_size=batch_size,\n",
    "                      callbacks=[checkpoint, earlystopping, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('BLSTM.h5')\n",
    "y_pred = model.predict_classes(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('BLSTM_result.txt', 'w') as f:\n",
    "    for i,y in enumerate(y_pred):\n",
    "        f.write(str(i+8001) + '\\t' + inf_classes[y] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attBLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    #input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(86, activation='softmax')(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n",
    "    return output_attention_mul\n",
    "\n",
    "# build RNN model with attention\n",
    "inputs = Input(shape=(86, 300))\n",
    "drop1 = Dropout(0.3)(inputs)\n",
    "lstm_out = Bidirectional(LSTM(units=128, unit_forget_bias=True, implementation=2, activation='tanh', recurrent_activation='hard_sigmoid',kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', return_sequences=True), name='bilstm')(drop1)\n",
    "attention_mul = attention_3d_block(lstm_out)\n",
    "attention_flatten = Flatten()(attention_mul)\n",
    "drop2 = Dropout(0.3)(attention_flatten)\n",
    "output = Dense(19, activation='softmax')(drop2)\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile('RMSprop', 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 86, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 86, 300)      0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bilstm (Bidirectional)          (None, 86, 256)      439296      dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "permute_4 (Permute)             (None, 256, 86)      0           bilstm[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 256, 86)      7482        permute_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 86, 256)      0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Multiply)        (None, 86, 256)      0           bilstm[0][0]                     \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 22016)        0           attention_mul[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 22016)        0           flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 19)           418323      dropout_33[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 865,101\n",
      "Trainable params: 865,101\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 2.7203 - acc: 0.1756Epoch 00001: val_acc improved from -inf to 0.17000, saving model to attBLSTM.h5\n",
      "7200/7200 [==============================] - 113s 16ms/step - loss: 2.7208 - acc: 0.1749 - val_loss: 2.6455 - val_acc: 0.1700\n",
      "Epoch 2/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 2.6048 - acc: 0.1957Epoch 00002: val_acc improved from 0.17000 to 0.23000, saving model to attBLSTM.h5\n",
      "7200/7200 [==============================] - 97s 13ms/step - loss: 2.6043 - acc: 0.1958 - val_loss: 2.5184 - val_acc: 0.2300\n",
      "Epoch 3/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 2.4541 - acc: 0.2356Epoch 00003: val_acc improved from 0.23000 to 0.24125, saving model to attBLSTM.h5\n",
      "7200/7200 [==============================] - 98s 14ms/step - loss: 2.4544 - acc: 0.2354 - val_loss: 2.3385 - val_acc: 0.2412\n",
      "Epoch 4/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 2.3162 - acc: 0.2723Epoch 00004: val_acc did not improve\n",
      "7200/7200 [==============================] - 99s 14ms/step - loss: 2.3165 - acc: 0.2715 - val_loss: 2.3857 - val_acc: 0.2112\n",
      "Epoch 5/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 2.2022 - acc: 0.2966Epoch 00005: val_acc improved from 0.24125 to 0.34375, saving model to attBLSTM.h5\n",
      "7200/7200 [==============================] - 98s 14ms/step - loss: 2.2025 - acc: 0.2967 - val_loss: 2.1116 - val_acc: 0.3438\n",
      "Epoch 6/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 2.0879 - acc: 0.3273Epoch 00006: val_acc did not improve\n",
      "7200/7200 [==============================] - 99s 14ms/step - loss: 2.0862 - acc: 0.3281 - val_loss: 2.0527 - val_acc: 0.3362\n",
      "Epoch 7/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 2.0014 - acc: 0.3492Epoch 00007: val_acc improved from 0.34375 to 0.39750, saving model to attBLSTM.h5\n",
      "7200/7200 [==============================] - 99s 14ms/step - loss: 2.0002 - acc: 0.3497 - val_loss: 1.9099 - val_acc: 0.3975\n",
      "Epoch 8/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.9272 - acc: 0.3670Epoch 00008: val_acc did not improve\n",
      "7200/7200 [==============================] - 96s 13ms/step - loss: 1.9277 - acc: 0.3672 - val_loss: 1.8235 - val_acc: 0.3925\n",
      "Epoch 9/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.8705 - acc: 0.3848Epoch 00009: val_acc improved from 0.39750 to 0.42500, saving model to attBLSTM.h5\n",
      "7200/7200 [==============================] - 97s 14ms/step - loss: 1.8710 - acc: 0.3846 - val_loss: 1.8061 - val_acc: 0.4250\n",
      "Epoch 10/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.8188 - acc: 0.4047Epoch 00010: val_acc improved from 0.42500 to 0.43375, saving model to attBLSTM.h5\n",
      "7200/7200 [==============================] - 104s 14ms/step - loss: 1.8182 - acc: 0.4047 - val_loss: 1.7145 - val_acc: 0.4338\n",
      "Epoch 11/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.7678 - acc: 0.4145Epoch 00011: val_acc improved from 0.43375 to 0.43625, saving model to attBLSTM.h5\n",
      "7200/7200 [==============================] - 95s 13ms/step - loss: 1.7684 - acc: 0.4144 - val_loss: 1.7380 - val_acc: 0.4363\n",
      "Epoch 12/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.7156 - acc: 0.4298Epoch 00012: val_acc improved from 0.43625 to 0.45250, saving model to attBLSTM.h5\n",
      "7200/7200 [==============================] - 92s 13ms/step - loss: 1.7147 - acc: 0.4304 - val_loss: 1.6751 - val_acc: 0.4525\n",
      "Epoch 13/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.6750 - acc: 0.4446Epoch 00013: val_acc did not improve\n",
      "7200/7200 [==============================] - 92s 13ms/step - loss: 1.6749 - acc: 0.4444 - val_loss: 1.6822 - val_acc: 0.4525\n",
      "Epoch 14/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.6275 - acc: 0.4590Epoch 00014: val_acc did not improve\n",
      "7200/7200 [==============================] - 92s 13ms/step - loss: 1.6280 - acc: 0.4592 - val_loss: 1.8132 - val_acc: 0.4163\n",
      "Epoch 15/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.6017 - acc: 0.4682Epoch 00015: val_acc did not improve\n",
      "7200/7200 [==============================] - 92s 13ms/step - loss: 1.6016 - acc: 0.4679 - val_loss: 1.7092 - val_acc: 0.4412\n",
      "Epoch 16/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.5598 - acc: 0.4773Epoch 00016: val_acc improved from 0.45250 to 0.47750, saving model to attBLSTM.h5\n",
      "7200/7200 [==============================] - 92s 13ms/step - loss: 1.5595 - acc: 0.4774 - val_loss: 1.5808 - val_acc: 0.4775\n",
      "Epoch 17/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.5266 - acc: 0.4916Epoch 00017: val_acc improved from 0.47750 to 0.51125, saving model to attBLSTM.h5\n",
      "7200/7200 [==============================] - 92s 13ms/step - loss: 1.5272 - acc: 0.4917 - val_loss: 1.5032 - val_acc: 0.5112\n",
      "Epoch 18/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.4946 - acc: 0.5017Epoch 00018: val_acc did not improve\n",
      "7200/7200 [==============================] - 92s 13ms/step - loss: 1.4954 - acc: 0.5014 - val_loss: 1.8855 - val_acc: 0.4075\n",
      "Epoch 19/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.4913 - acc: 0.5071Epoch 00019: val_acc improved from 0.51125 to 0.52250, saving model to attBLSTM.h5\n",
      "7200/7200 [==============================] - 92s 13ms/step - loss: 1.4915 - acc: 0.5071 - val_loss: 1.5079 - val_acc: 0.5225\n",
      "Epoch 20/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.4469 - acc: 0.5201Epoch 00020: val_acc did not improve\n",
      "7200/7200 [==============================] - 92s 13ms/step - loss: 1.4480 - acc: 0.5203 - val_loss: 1.4911 - val_acc: 0.4950\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 20\n",
    "batch_size = 256\n",
    "save_path = 'attBLSTM.h5'\n",
    "checkpoint = ModelCheckpoint(filepath=save_path,\n",
    "                                     verbose=1,\n",
    "                                     save_best_only=True,\n",
    "                                     save_weights_only=False,\n",
    "                                     monitor='val_acc',\n",
    "                                     mode='max' )\n",
    "csv_logger = CSVLogger('%s-log.csv'%'attBLSTM', separator=',', append=False)\n",
    "earlystopping = EarlyStopping(monitor='val_acc', patience = 4, verbose=1, mode='max')\n",
    "history = model.fit(X, Y,\n",
    "                      validation_data=(X_val, Y_val),\n",
    "                      epochs=nb_epoch,\n",
    "                      batch_size=batch_size,\n",
    "                      callbacks=[checkpoint, earlystopping, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.4123 - acc: 0.5345Epoch 00001: val_acc did not improve\n",
      "7200/7200 [==============================] - 102s 14ms/step - loss: 1.4117 - acc: 0.5347 - val_loss: 1.5092 - val_acc: 0.5162\n",
      "Epoch 2/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.3813 - acc: 0.5451Epoch 00002: val_acc improved from 0.52250 to 0.52625, saving model to attBLSTM.h5\n",
      "7200/7200 [==============================] - 100s 14ms/step - loss: 1.3791 - acc: 0.5461 - val_loss: 1.4386 - val_acc: 0.5262\n",
      "Epoch 3/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.3525 - acc: 0.5485Epoch 00003: val_acc improved from 0.52625 to 0.54250, saving model to attBLSTM.h5\n",
      "7200/7200 [==============================] - 109s 15ms/step - loss: 1.3540 - acc: 0.5483 - val_loss: 1.4105 - val_acc: 0.5425\n",
      "Epoch 4/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.3186 - acc: 0.5607Epoch 00004: val_acc did not improve\n",
      "7200/7200 [==============================] - 104s 14ms/step - loss: 1.3191 - acc: 0.5607 - val_loss: 1.4387 - val_acc: 0.5212\n",
      "Epoch 5/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.3001 - acc: 0.5684Epoch 00005: val_acc did not improve\n",
      "7200/7200 [==============================] - 103s 14ms/step - loss: 1.3007 - acc: 0.5679 - val_loss: 1.4029 - val_acc: 0.5425\n",
      "Epoch 6/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.2715 - acc: 0.5737Epoch 00006: val_acc did not improve\n",
      "7200/7200 [==============================] - 104s 14ms/step - loss: 1.2708 - acc: 0.5737 - val_loss: 1.4487 - val_acc: 0.5088\n",
      "Epoch 7/20\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 1.2545 - acc: 0.5797Epoch 00007: val_acc did not improve\n",
      "7200/7200 [==============================] - 105s 15ms/step - loss: 1.2544 - acc: 0.5800 - val_loss: 1.4043 - val_acc: 0.5162\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "csv_logger = CSVLogger('%s-log.csv'%'attBLSTM_cont', separator=',', append=False)\n",
    "history = model.fit(X, Y,\n",
    "                      validation_data=(X_val, Y_val),\n",
    "                      epochs=nb_epoch,\n",
    "                      batch_size=batch_size,\n",
    "                      callbacks=[checkpoint, earlystopping, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = load_model('attBLSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_3.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for y in y_pred:\n",
    "    result.append(np.argmax(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('attBLSTM_result.txt', 'w') as f:\n",
    "    for i,y in enumerate(result):\n",
    "        f.write(str(i+8001) + '\\t' + inf_classes[y] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
